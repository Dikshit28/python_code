{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    " Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "Point estimates and confidence intervals are basic inference tools that act as the foundation for another inference technique: statistical hypothesis testing. Statistical hypothesis testing is a framework for determining whether observed data deviates from what is expected. Python's scipy.stats library contains an array of functions that make it easy to carry out hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical hypothesis tests are based a statement called the null hypothesis that assumes nothing interesting is going on between whatever variables you are testing. The exact form of the null hypothesis varies from one type test to another: if you are testing whether groups differ, the null hypothesis states that the groups are the same. For instance, if you wanted to test whether the average age of voters in your home state differs from the national average, the null hypothesis would be that there is no difference between the average ages.\n",
    "\n",
    "The purpose of a hypothesis test is to determine whether the null hypothesis is likely to be true given sample data. If there is little evidence against the null hypothesis given the data, you accept the null hypothesis. If the null hypothesis is unlikely given the data, you might reject the null in favor of the alternative hypothesis: that something interesting is going on. The exact form of the alternative hypothesis will depend on the specific test you are carrying out. Continuing with the example above, the alternative hypothesis would be that the average age of voters in your state does in fact differ from the national average.\n",
    "\n",
    "Once you have the null and alternative hypothesis in hand, you choose a significance level (often denoted by the Greek letter α.). The significance level is a probability threshold that determines when you reject the null hypothesis. After carrying out a test, if the probability of getting a result as extreme as the one you observe due to chance is lower than the significance level, you reject the null hypothesis in favor of the alternative. This probability of seeing a result as extreme or more extreme than the one observed is known as the p-value.\n",
    "\n",
    "The T-test is a statistical test used to determine whether a numeric data sample of differs significantly from the population or whether two samples differ from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Sample T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A one-sample t-test checks whether a sample mean differs from the population mean. Let's create some dummy age data for the population of voters in the entire country and a sample of voters in Minnesota and test the whether the average age of voters Minnesota differs from the population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.000112\n",
      "39.26\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "\n",
    "population_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000)\n",
    "population_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000)\n",
    "population_ages = np.concatenate((population_ages1, population_ages2))\n",
    "\n",
    "minnesota_ages1 = stats.poisson.rvs(loc=18, mu=30, size=30)\n",
    "minnesota_ages2 = stats.poisson.rvs(loc=18, mu=10, size=20)\n",
    "minnesota_ages = np.concatenate((minnesota_ages1, minnesota_ages2))\n",
    "\n",
    "print( population_ages.mean() )\n",
    "print( minnesota_ages.mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used a slightly different combination of distributions to generate the sample data for Minnesota, so we know that the two means are different. Let's conduct a t-test at a 95% confidence level and see if it correctly rejects the null hypothesis that the sample comes from the same distribution as the population. To conduct a one sample t-test, we can the stats.ttest_1samp() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=-2.5742714883655027, pvalue=0.013118685425061678)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_1samp(a = minnesota_ages,               # Sample data\n",
    "                 popmean = population_ages.mean())  # Pop mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test result shows the test statistic \"t\" is equal to -2.574. This test statistic tells us how much the sample mean deviates from the null hypothesis. If the t-statistic lies outside the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we reject the null hypothesis. We can check the quantiles with stats.t.ppf():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0095752344892093\n"
     ]
    }
   ],
   "source": [
    "t1=stats.t.ppf(q=0.025,df=49)  # Degrees of freedom\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.009575234489209\n"
     ]
    }
   ],
   "source": [
    "t2=stats.t.ppf(q=0.975,df=49)  # Degrees of freedom\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can calculate the chances of seeing a result as extreme as the one we observed (known as the p-value) by passing the t-statistic in as the quantile to the stats.t.cdf() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013121066545690117"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.t.cdf(x= -2.5742,df= 49) * 2   # Multiply by two for two tailed test *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The alternative hypothesis we are checking is whether the sample mean differs (is not equal to) the population mean. Since the sample could differ in either the positive or negative direction we multiply the by two.*\n",
    "\n",
    "Notice this value is the same as the p-value listed in the original t-test output. A p-value of 0.01311 means we'd expect to see data as extreme as our sample due to chance about 1.3% of the time if the null hypothesis was true. In this case, the p-value is lower than our significance level α (equal to 1-conf.level or 0.05) so we should reject the null hypothesis. If we were to construct a 95% confidence interval for the sample it would not capture population mean of 43:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36.369669080722176, 42.15033091927782)\n"
     ]
    }
   ],
   "source": [
    "sigma = minnesota_ages.std()/math.sqrt(50)  # Sample stdev/sample size\n",
    "\n",
    "interval_range=stats.t.interval(0.95, df = 49,loc = minnesota_ages.mean(),scale= sigma)                # Standard dev estimate\n",
    "print(interval_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, since there is a 1.3% chance of seeing a result this extreme due to chance, it is not significant at the 99% confidence level. This means if we were to construct a 99% confidence interval, it would capture the population mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.t.interval(alpha = 0.99,                # Confidence level\n",
    "                 df = 49,                     # Degrees of freedom\n",
    "                 loc = minnesota_ages.mean(), # Sample mean\n",
    "                 scale= sigma)                # Standard dev estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher confidence level, we construct a wider confidence interval and increase the chances that it captures to true mean, thus making it less likely that we'll reject the null hypothesis. In this case, the p-value of 0.013 is greater than our significance level of 0.01 and we fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Sample T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A two-sample t-test investigates whether the means of two independent data samples differ from one another. In a two-sample test, the null hypothesis is that the means of both groups are the same. Unlike the one sample-test where we test against a known population parameter, the two sample test only involves sample means. You can conduct a two-sample t-test by passing with the stats.ttest_ind() function. Let's generate a sample of voter age data for Wisconsin and test it against the sample we made earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "wisconsin_ages1 = stats.poisson.rvs(loc=18, mu=33, size=30)\n",
    "wisconsin_ages2 = stats.poisson.rvs(loc=18, mu=13, size=20)\n",
    "wisconsin_ages = np.concatenate((wisconsin_ages1, wisconsin_ages2))\n",
    "\n",
    "print( wisconsin_ages.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(a= minnesota_ages,\n",
    "                b= wisconsin_ages,\n",
    "                equal_var=False)    # Assume samples have equal variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test yields a p-value of 0.0907, which means there is a 9% chance we'd see sample data this far apart if the two groups tested are actually identical. If we were using a 95% confidence level we would fail to reject the null hypothesis, since the p-value is greater than the corresponding significance level of 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired T-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic two sample t-test is designed for testing differences between independent groups. In some cases, you might be interested in testing differences between samples of the same group at different points in time. For instance, a hospital might want to test whether a weight-loss drug works by checking the weights of the same group patients before and after treatment. A paired t-test lets you check whether the means of samples from the same group differ.\n",
    "\n",
    "We can conduct a paired t-test using the scipy function stats.ttest_rel(). Let's generate some dummy patient weight data and do a paired t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "before= stats.norm.rvs(scale=30, loc=250, size=100)\n",
    "\n",
    "after = before + stats.norm.rvs(scale=5, loc=-1.25, size=100)\n",
    "\n",
    "weight_df = pd.DataFrame({\"weight_before\":before,\n",
    "                          \"weight_after\":after,\n",
    "                          \"weight_change\":after-before})\n",
    "\n",
    "weight_df.describe()             # Check a summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary shows that patients lost about 1.23 pounds on average after treatment. Let's conduct a paired t-test to see whether this difference is significant at a 95% confidence level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(a = before,\n",
    "                b = after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type I and Type II Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of a statistical hypothesis test and the corresponding decision of whether to reject or accept the null hypothesis is not infallible. A test provides evidence for or against the null hypothesis and then you decide whether to accept or reject it based on that evidence, but the evidence may lack the strength to arrive at the correct conclusion. Incorrect conclusions made from hypothesis tests fall in one of two categories: type I error and type II error.\n",
    "\n",
    "Type I error describes a situation where you reject the null hypothesis when it is actually true. This type of error is also known as a \"false positive\" or \"false hit\". The type 1 error rate is equal to the significance level α, so setting a higher confidence level (and therefore lower alpha) reduces the chances of getting a false positive.\n",
    "\n",
    "Type II error describes a situation where you fail to reject the null hypothesis when it is actually false. Type II error is also known as a \"false negative\" or \"miss\". The higher your confidence level, the more likely you are to make a type II error.\n",
    "\n",
    "Let's investigate these errors with a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "\n",
    "plt.fill_between(x=np.arange(-4,-2,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(-4,-2,0.01)) ,\n",
    "                 facecolor='red',\n",
    "                 alpha=0.35)\n",
    "\n",
    "plt.fill_between(x=np.arange(-2,2,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(-2,2,0.01)) ,\n",
    "                 facecolor='grey',\n",
    "                 alpha=0.35)\n",
    "\n",
    "plt.fill_between(x=np.arange(2,4,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(2,4,0.01)) ,\n",
    "                 facecolor='red',\n",
    "                 alpha=0.5)\n",
    "\n",
    "plt.fill_between(x=np.arange(-4,-2,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(-4,-2,0.01),loc=3, scale=2) ,\n",
    "                 facecolor='grey',\n",
    "                 alpha=0.35)\n",
    "\n",
    "plt.fill_between(x=np.arange(-2,2,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(-2,2,0.01),loc=3, scale=2) ,\n",
    "                 facecolor='blue',\n",
    "                 alpha=0.35)\n",
    "\n",
    "plt.fill_between(x=np.arange(2,10,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(2,10,0.01),loc=3, scale=2),\n",
    "                 facecolor='grey',\n",
    "                 alpha=0.35)\n",
    "\n",
    "plt.text(x=-0.8, y=0.15, s= \"Null Hypothesis\")\n",
    "plt.text(x=2.5, y=0.13, s= \"Alternative\")\n",
    "plt.text(x=2.1, y=0.01, s= \"Type 1 Error\")\n",
    "plt.text(x=-3.2, y=0.01, s= \"Type 1 Error\")\n",
    "plt.text(x=0, y=0.02, s= \"Type 2 Error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, the red areas indicate type I errors assuming the alternative hypothesis is not different from the null for a two-sided test with a 95% confidence level.\n",
    "\n",
    "The blue area represents type II errors that occur when the alternative hypothesis is different from the null, as shown by the distribution on the right. Note that the Type II error rate is the area under the alternative distribution within the quantiles determined by the null distribution and the confidence level. We can calculate the type II error rate for the distributions above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quantile = stats.norm.ppf(0.025)  # Lower cutoff value\n",
    "upper_quantile = stats.norm.ppf(0.975)  # Upper cutoff value\n",
    "\n",
    "# Area under alternative, to the left the lower cutoff value\n",
    "low = stats.norm.cdf(lower_quantile,    \n",
    "                     loc=3,             \n",
    "                     scale=2)\n",
    "\n",
    "# Area under alternative, to the left the upper cutoff value\n",
    "high = stats.norm.cdf(upper_quantile, \n",
    "                      loc=3, \n",
    "                      scale=2)          \n",
    "\n",
    "# Area under the alternative, between the cutoffs (Type II error)\n",
    "high-low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the normal distributions above, we'd fail to reject the null hypothesis about 30% of the time because the distributions are close enough together that they have significant overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [power](https://en.wikipedia.org/wiki/Power_(statistics)) of a statistical test is the probability that the test rejects the null hypothesis when the alternative is actually different from the null. In other words, power is the probability that the test detects that there is something interesting going on when there actually *is* something interesting going on. Power is equal to one minus the type II error rate. The power of a statistical test is influenced by:\n",
    "\n",
    "1. The significance level chosen for the test.\n",
    "2. The sample size.\n",
    "3. The effect size of the test.\n",
    "\n",
    "When choosing a significance level for a test, there is a trade-off between type I and type II error. A low significance level, such as 0.01 makes a test unlikely to have type I errors (false positives), but more likely to have type II errors (false negatives) than a test with larger value of the significance level α. A common convention is that a statistical tests should have a power of at least 0.8.\n",
    "\n",
    "A larger sample size reduces the uncertainty of the point estimate, causing the sample distribution to narrow, resulting in lower type II error rates and increased power.\n",
    "\n",
    "[Effect size](https://en.wikipedia.org/wiki/Effect_size) is a general term that describes a numeric measure of the size of some phenomenon. There are many different effect size measurements that arise in different contexts. In the context of the T-test, a simple effect size is the difference between the means of the samples. This number can be standardized by dividing by the standard deviation of the population or the pooled standard deviation of the samples. This puts the size of the effect in terms of standard deviations, so a standardized effect size of 0.5 would be interpreted as one sample mean being 0.5 standard deviations from another (in general 0.5 is considered a \"large\" effect size).\n",
    "\n",
    "Since statistical power, the significance level, the effect size and the sample size are related, it is possible to calculate any one of them for given values of the other three. This can be an important part of the process of designing a hypothesis test and analyzing results. For instance, if you want to conduct a test with a given significance level (say the standard 0.05) and power (say the standard 0.8) and you are interested in a given effect size (say 0.5 for standardized difference between sample means), you could use that information to determine how large of a sample size you need.\n",
    "\n",
    "In python, the statsmodels library contains functions to solve for any one parameter of the power of T-tests. Use statsmodels.stats.power.tt_solve_power for one sample t-tests and statsmodels.stats.power.tt_ind_solve_power for a two sample t-test. Let's check the sample size we should use need to use given the standard parameter values above for a one sample t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import tt_solve_power\n",
    "\n",
    "tt_solve_power(effect_size = 0.5,\n",
    "               alpha = 0.05,\n",
    "               power = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we would want a sample size of at least 34 to make a study with the desired power and signifiance level capable of detecting a large effect size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
